# E1.2 - Errors and error propagation

Ces exercices se focalisent sur les erreurs (comme la fin du précédent TP) en rentrant davantage dans les détails.

## Question 1 - Truncation errors for series

How many term would we need to sum in the following series to garantee a trucation error bellow $10^{-10}$:

$$
\begin{align}
\sin(x) &= x − \frac{x^3}{3!} + \frac{x^5}{5!} − … \text{for }x \in [−\pi, \pi] \\
\exp(x) &= 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + … \text{for }x \in [−1, 1] \\
\ln(1 + x) &= x − \frac{x^2}{2} + \frac{x^3}{3} − \frac{x^4}{4} + … \text{for }x \in [0, 0.99]
\end{align}
$$

Your answer should have the following format: `r1, r2, r3`

**Hint**: create a small python script to compute these series with a predifined number $n$ of terms and compare the result with the actual values of $\sin$, $\exp$ and $\ln$.

### Solution

```
11, 14, 1495
```

### Explications

Il suffit, comme l'explique l'indice, de faire un script Python pour chaque cas afin de calculer à partir de combien de terme la différence entre la valeur réelle et l'approximation passe en dessous de $10^{-10}$.

Pour $\sin(x)$, on peut utiliser le code suivant :

```py
import numpy as np

def sin_series(x, n):
    result = 0
    sign = 1

    for i in range(1, 2 * n, 2):
        result += sign * (x ** i) / np.math.factorial(i)
        sign *= -1 # On inverse le signe

    return result

x = np.arange(-np.pi, np.pi + 0.01, 0.01) # Le deuxième argument est exclusif. On doit donc mettre np.pi + 0.1 pour être sur que pi soit dans le vecteur
n = 1

while not np.allclose(np.sin(x), np.vectorize(sin_series)(x, n), atol=1e-10, rtol=0):
    n += 1

print(f"Le n minimal est {n}.")
```

On obtient alors comme résultat que $n = 11$.

Pour $\exp(x)$, on répète le même processus :

```py
import numpy as np

def exp_series(x, n):
    result = 0

    for i in range(n):
        result += (x ** i) / np.math.factorial(i)

    return result

x = np.arange(-1, 1 + 0.01, 0.01)
n = 1

while not np.allclose(np.vectorize(exp_series)(x, n), np.exp(x), atol=1e-10, rtol=0):
    n += 1

print(f"Le n minimal est {n}.")
```

On obtient alors comme résultat $n = 14$.

Encore la même chose avec $\ln(x)$ :

```py
import numpy as np

def ln_series(x, n):
    result = 0
    sign = 1

    for i in range(1, n + 1):
        result += sign * (x ** i) / i
        sign *= -1 # On inverse le signe

    return result

x = np.arange(0, 0.99 + 0.01, 0.01)
n = 1

while not np.allclose(np.vectorize(ln_series)(x, n), np.log(1 + x), atol=1e-10, rtol=0):
    n += 1

print(f"Le n minimal est {n}.")
```

On obtient alors comme résultat $n = 1495$.

::: tip Précisions
Les fonctions [`np.allclose`](https://numpy.org/doc/stable/reference/generated/numpy.allclose.html) et [`np.vectorize`](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html) sont toutes les deux utilisées dans ces codes et sont très intéressantes. La première permet de vérifier si deux _arrays_ ont tous leurs éléments égaux à une certaine tolérance près (définie par l'argument nommé `atol`. `rtol` quant à lui ne nous intéresse pas et doit donc être défini à 0, au risque de produire des résultats erronés). `np.sin` renvoie bien un _array_, mais ce n'est pas le cas pour la fonction `sin_series`. `np.vectorize` permet de réparer facilement le problème : elle prend une fonction `A` en argument et renvoie une autre fonction (que l'on appelle directement avec les arguments `x` et `n`) qui peut cette fois-ci prendre un _array_ en argument et renvoyer un nouvel _array_ dans lequel se trouveront tous les éléments auxquels on aura appliqué la fonction `A`. (Cela est équivalent à la fonction `map` d'une certaine manière)
:::

## Question 2 - Absolute and relative errors (1/4)

Assume that $\tilde{x}=0.937$ has three exact decimals with respect to $x$, meaning that $x \in [0.9365, 0.9375]$.

-   What is the (maximal) absolute error on $x$?
-   What is the (maximal) relative error on $x$ in %?

_Your answer should have the following format:_ `r1, r2`

### Solution

```
0.0005, 0.05
```

### Explications

::: info Rappel
L'erreur absolue est donnée par $e_x \le |\tilde{x} - x|$ et l'erreur relative par $\epsilon_x \le \frac{|\tilde{x} - x|}{|x|}$, où dans les deux cas, $x$ correspond à la valeur réelle et $\tilde{x}$ à l'approximation (avec par conséquent $\tilde{x} = x + e_x$).
:::

Pour calculer l'erreur absolue maximale entre $x$ et $\tilde{x}$, il faudrait en théorie appliquer la formule sur toutes les valeurs possibles de $x$. Dans la pratique néanmoins, on sait que les valeurs éloignées de l'approximation produisent généralement de plus grandes erreurs. On peut donc prendre $x = 0.9365$ ou $x = 0.9375$, étant donné que ces valeurs sont éloignées de la même façon de $\tilde{x}$. On a donc :

$$
\begin{align}
e_x \le |0.937 - 0.9365| = 0.0005 && e_x \le |0.937 - 0.9375| = 0.0005
\end{align}
$$

L'erreur absolue maximale est donc de $0.0005$ avec $x = 0.9365$ ou $x = 0.9375$.

Le même principe est appliqué pour calculer l'erreur relative maximale. On a donc :

$$
\begin{align}
\epsilon_x \le \frac{|0.937 - 0.9365|}{|0.9365|} \simeq 0.0005 && \epsilon_x \le \frac{|0.937 - 0.9375|}{|0.9375|} \simeq 0.0005
\end{align}
$$

L'erreur relative maximale est donc de $0.0005 = 0.05\%$ avec $x = 0.9365$ ou $x = 0.9375$.

## Question 3 - Absolute and relative errors (2/4)

Suppose that $f(x) = \sqrt{1 − x}$ and $\tilde{x} = 0.937$.

-   What is the (maximal) absolute error on $f(x)$?
-   What is the (maximal) relative error on $f(x)$ in %?

_Your answer should have the following format:_ `r1, r2`

### Solution

```
0.001, 0.4
```

### Explications

::: info Rappel
L'erreur absolue d'une fonction est donnée par $e_f \le |\frac{\partial f(x)}{\partial x}| * |e_x|$ et l'erreur relative par $\epsilon_x \le \frac{e_f}{f(x)}$.
:::

Pour calculer l'erreur absolue maximale, il faudrait appliquer la formule pour chaque valeur de $x$. Néanmoins, comme expliqué dans la question précédente, les valeurs de $x$ éloignées de l'approximation donnent généralement une plus grande erreur. Nous allons donc prendre $x = 0.9365$ et $x = 0.9375$ :

$$
\begin{align}
e_f &\le |- \frac{1}{2\sqrt{1 - 0.9365}}| * 0.0005 & e_f &\le |- \frac{1}{2\sqrt{1 - 0.9375}}| * 0.0005\\
&\simeq 0.001 & &= 0.001
\end{align}
$$

On obtient une réponse légèrement inférieure à $0.01$ avec $x = 0.9365$. L'erreur maximale absolue est donc de $0.001$ avec $x = 0.9375$.

Le même principe est appliqué pour calculer l'erreur relative maximale. On va cette fois-ci uniquement le faire avec $x=0.9375$, étant donné que c'est celui qui donne la plus grande erreur avec cette fonction. On a donc :

$$\epsilon_x = \frac{0.001}{\sqrt{1 - 0.9375}} = \frac{0.001}{0.25} = 0.004$$

L'erreur relative maximale est donc de $0.004 = 0.4\%$ avec $x = 0.9375$.

## Question 4 - Absolute and relative errors (3/4)

Now, for the same function, assume that $\tilde{x} = 0.999$ and still has three exact decimals with respect to $x$.

-   What is the (maximal) absolute error on $f(x)$?
-   What is the (maximal) relative error on $f(x)$ in %?

_Your answer should have the following format:_ `r1, r2`

### Solution

```
0.0111, 50
```

### Explications

Comme $\tilde{x}$ a changé, l'intervalle de $x$ également ($x \in [0.9985; 0.9995]$).

On réapplique les mêmes formules qu'avant, avec cette fois-ci $x = 0.99985$ et $x = 0.9995$ :

$$
\begin{align}
&e_x \le |0.999 - 0.9985| = 0.0005 &&e_x \le |0.999 - 0.9995| = 0.0005\\
&e_f \le |- \frac{1}{2\sqrt{1 - 0.9985}}| * 0.0005 &&e_f \le |- \frac{1}{2\sqrt{1 - 0.9995}}| * 0.0005\\
&\simeq 0.0065 &&\simeq 0.0111
\end{align}
$$

L'erreur absolue maximale est $0.0111$ avec $x = 0.9995$.

Pour l'erreur relative maximale, on calcule :

$$\epsilon_x = \frac{0.0111}{\sqrt{1 - 0.9995}} = \frac{0.0111}{0.0224} = 0.496 \simeq 0.5$$

L'erreur relative maximale est donc $0.5 = 50\%$ avec $x = 0.9995$.

## Question 5 - Absolute and relative errors (4/4)

Use _matplotlib_ to plot on the same figure:

-   $f(x)$
-   $f(x) + |e_f(x)|$
-   $f(x) − |e_f(x)|$

for $x \in [0.97, 1[$.

Use increments of $2 * 10^{−5}$ for your vector `x` and assume a constant $e_x = 10^{-3}$. Store the values of $f(x)$ and $e_f(x)$ in variables `f` and `e`.

### Solution

Comme pour tous les énoncés faisant intervenir Matplotlib, la solution est en deux parties :

-   Le code (qui est à soumettre sur INGInious)
-   Le graphe (qui n'est pas corrigé par INGInious et qui est là à titre indicatif).

#### Code

```py:line-numbers
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(0.97, 1 - 2e-5, 2e-5)
f = np.sqrt(1 - x)
e = np.abs(-1 / (2 * np.sqrt(1 - x))) * 1e-3

plt.plot(x, f, "r", label="f(x)")
plt.plot(x, f + e, "g", label="f(x) + |e_f|")
plt.plot(x, f - e, "b", label="f(x) - |e_f|")

plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.show()
```

#### Graphe

![Graphe Matplotlib Q5](/images/e1-2-q5.png)

### Explications

Il suffit de créer le vecteur demandé avec la [fonction `arange`](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) et de le stocker dans `x`, puis de transformer la fonction et l'erreur absolue en Python, ce qui est fait avec Numpy.

::: danger Attention
Dans ce code précis, vous remarquerez sûrement que `arange` ne fonctionne pas de la bonne façon : le deuxième paramètre est censé être exclu de l'_array_ produit. Néanmoins, si on met `1` au lieu de `1 - 2e-5`, la valeur 1 se retrouve dans l'_array_. Cela est dû à des erreurs causées par les nombres à virgules lorsque la prochaine valeur est trop proche de la précédente (comme ici, où l'on travaille avec des pas de $2 * 10^{-5}$). Il faut donc enlever l'équivalent d'un pas à la valeur, ce qu'on fait dans la fonction.
:::

On utilise ensuite la [fonction `plot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) pour tracer la fonction $f(x)$, la fonction $f(x) + |e_x|$ et la fonction $f(x) - |e_x|$. Les fonctions [`xlabel`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html), [`ylabel`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylabel.html) et [`legend`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html) sont utilisées pour respectivement donner un nom à l'axe des abscisses, donner un nom à l'axe des ordonnées et afficher la légende (en haut à droite sur le [graphe](#graphe)). Pour finir, on appelle la [fonction `show`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html) pour afficher le graphe.

## Question 6 - Error propagation on functions

How would you propagate the error $e_x$, $e_y$ and $e_z$ associated to $x$, $y$ and $z$ through the function:

$$
f(x, y, z) = \sin(x) * y^2 + \frac{z^3}{y} − x + xyz
$$

We expect an answer of the form: $|e_f| \leq$ `[your formula]`

Write your formula in a function `error(x, y, z, e_x, e_y, e_z)` that returns the value of the propagation, given the values of $x$, $y$, $z$, $e_x$, $e_y$ and $e_z$. Your function must respect $|e_f| \leq$ `error(...)`

**Hint**: You can use the functions `np.abs` from numpy to compute absolute values, `np.cos` and `np.sin` for trigonometric functions, and `a**b` to compute $a^b$

### Solution

```py:line-numbers
import numpy as np

def error(x, y, z, e_x, e_y, e_z):
    return np.abs(y ** 2 * np.cos(x) - 1 + y * z) * np.abs(e_x) + np.abs(2 * y * np.sin(x) - z ** 3 / y ** 2 + x * z) * np.abs(e_y) + np.abs(3 * z ** 2 / y + x * y) * np.abs(e_z)
```

### Explications

::: info Rappel
L'erreur absolue d'une fonction à plus d'une variable est donnée par la formule suivante :
$$e_x \le \sum_{i=1}^n |\frac{\partial f}{\partial x_i} (x_0, ..., x_n)| * |e_i| $$
:::

La formule présentée ci-dessus peut paraître compliquée, mais elle est simple à appliquer. Il suffit d'appliquer la formule classique pour une fonction à une variable à chaque variable et d'additionner le tout. Étape par étape, cela donne :

$$
\begin{align}
e_{f(x)} &\le |\frac{\partial f(x, y, z)}{\partial x}| * |e_x|\\
&\le |y^2 cos(x) - 1 + yz| * |e_x|\\\\

e_{f(y)} &\le |\frac{\partial f(x, y, z)}{\partial y}| * |e_x|\\
&\le |2y sin(x) - \frac{z^3}{y^2} + xz| * |e_y|\\\\

e_{f(z)} &\le |\frac{\partial f(x, y, z)}{\partial z}| * |e_x|\\
&\le |\frac{3z^2}{y} + xy| * |e_z|

\end{align}
$$

On additionne ensuite les fonctions obtenues pour avoir la fonction de l'erreur absolue :

$$e_{f} \le |y^2 cos(x) - 1 + yz| * |e_x| + |2y sin(x) - \frac{z^3}{y^2} + xz| * |e_y| + |\frac{3z^2}{y} + xy| * |e_z|$$

Il suffit ensuite de traduire cette formule en Python, ce qui est fait dans la solution.

## Question 7 - Variance computation (1/2)

You have a data set of $N$ points: $\{x_i|i = 1, ..., N\}$
.

You want to compute the variance ($V$) of the dataset. Knowing that the mean is computed as $\mu = \frac{1}{N} \sum_{i = 1}^N x_i$, which of the following formulas to compute the variance is the best to use if all the $x_i$ values are close to the mean?

-   $V_1 = (\frac{1}{N} \sum_{i = 1}^N x_i^2) - \mu^2$
-   $V_2 = \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2$

### Solution

$V_2 = \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2$

### Explications

La formule $V_1 = (\frac{1}{N} \sum_{i = 1}^N x_i^2) - \mu^2$ est problématique car elle soustrait deux nombres à virgule très similaires (La moyenne des carrés est proche du carré de la moyenne, autrement dit $\frac{1}{N} \sum_{i = 1}^N x_i^2 \simeq \mu^2$). On aura donc une perte de précision (cf. slide 64 du cours 1). $V_2$ est par conséquent préférable.

## Question 8 - Variance computation (2/2)

Test this with Python. Compute values `V1` and `V2` for $N = 7$ and $x_i = 10^{6.1} + i$ for $i \in \{1, …, 7\}$.

**Hint**: The difference is much more visible when using 32 bit values for your `x` vector instead of the default 64 bit representation. You can choose the representation using the `dtype = np.float32` as argument when defining an array. Don't hesitate to test the two to see what happens! Note that this may depend on how you computed your variances (the results might differ between implementations).

### Solution

```py:line-numbers
import numpy as np

x = 10 ** 6.1 + np.arange(1, 8, dtype=np.float32)
V1 = np.mean(x**2)- np.mean(x) ** 2
V2 = np.mean((x - np.mean(x)) ** 2)
```

### Explications

Cette question est une continuité de la question précédente, vu qu'elle va prouver la perte de précision dont $V1$ souffre. Il suffit de traduire en Python les deux formules.

::: tip Remarque
$\frac{1}{N} \sum_{i=1}^N x_i$ est défini comme la moyenne de $x_i$, tout comme $\mu$. On peut donc les remplacer tous les deux par la [fonction `mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html).
:::

Après exécution, on obtient que $V1 = 185178.109375$ alors que $V2 = 4$ (qui est la bonne réponse). On voit clairement que $V1$ a subit un _overflow_, ce qui a fait exploser sa valeur et donne par conséquent un résultat complètement erroné.

## Question 9 - Operations errors

**(a)** Do the error propagation on the following float operation (we did multiplication and subtraction in the class room, see slide 63 to 67). See how the absolute (resp. relative) error on the operation depends on the absolute (resp. relative) error of each input parameter.

-   $x \otimes y$
-   $x \oslash y$
-   $x \oplus y$
-   $x \ominus y$
-   $(x \oplus y) \oplus z$
-   $(x \oplus z) \oplus y$

**Hint**:

-   $fl(operation(x, y, z)) = operation(x, y, z)(1 + \epsilon)$
-   $fl(x) = x(1 + \epsilon')$
-   $fl(y) = y(1 + \epsilon'')$
-   $fl(z) = z(1 + \epsilon''')$

**(b)** If $x = 1$, $y = 10^{−10}$, $z = −1$ and we are working with a number representation in base 10 with 10 decimals ($p$ = 10), can you numerically evaluate the relative error on:

-   $(x \oplus y) \oplus z$
-   $(x \oplus z) \oplus y$

**Hint**: In our representation (base 10, $p$ = 10), $(x \oplus y) \oplus z = 0$ and $(x \oplus z) \oplus y = 10^{−10}$.

_Give the answers of part_ **(b)** _in % under the format:_ `r1, r2`

### Solution

```
100, 0
```

### Explications

L'exercice le plus long de cette séance (et souvent le plus confus), soyez prêts.

Pour le point (a), il faut calculer l'erreur des opérations de float. Cela se fait en deux longues étapes :

-   Appliquer les formules
-   Essayer d'isoler l'opération de base

::: info Rappel
En plus des formules données dans la consigne, il existe les suivantes :

$$
\begin{align*}
&x \oplus y = fl(fl(x) + fl(y)) &&x \ominus y = fl(fl(x) - fl(y)) \\
&x \otimes y = fl(fl(x) * fl(y)) &&x \oslash y = fl(fl(x) / fl(y))
\end{align*}
$$

avec $fl(x)$ la représentation en float de $x$.

:::

Commençons par $x \otimes y$. De par les formules, on sait que :

$$
x \otimes y = fl(fl(x) * fl(y)) = (fl(x) * fl(y))(1 + \epsilon) = (x(1 + \epsilon') * y(1 + \epsilon''))(1 + \epsilon)
$$

On cherche maintenant à isoler l'opération de base, à savoir $x * y$ :

$$
(x(1 + \epsilon') * y(1 + \epsilon''))(1 + \epsilon) = (x * y)(1 + \epsilon)(1 + \epsilon')(1 + \epsilon'')
$$

L'idéal pour connaître l'erreur, c'est d'avoir uniquement deux termes : d'un côté l'opération de base, de l'autre l'erreur. On va donc faire de la distributivité pour simplifier le tout (en sachant que $\epsilon_a * \epsilon_b \simeq 0$, on peut retirer toute erreur au carré sans trop impacter la réponse finale) :

$$
(x * y)(1 + \epsilon)(1 + \epsilon')(1 + \epsilon'') = (x * y)(1 + \epsilon' + \epsilon + \epsilon\epsilon')(1 + \epsilon'') = (x * y)(1 + \epsilon' + \epsilon)(1 + \epsilon'') = (x * y)(1 + \epsilon'' + \epsilon' + \epsilon'\epsilon'' + \epsilon + \epsilon\epsilon'') = (x * y)(1 + \epsilon + \epsilon' + \epsilon'')
$$

L'erreur est située dans le terme de droite duquel on exclue le 1 (celui-ci correspond à l'opération de base, l'erreur vient s'y ajouter). L'erreur de $x \otimes y$ est donc de $|\epsilon + \epsilon' + \epsilon''|$ (une erreur ne pouvant pas être négative, on garde bien la valeure absolue).

Nous allons faire cette propagation pour $x \oplus y$, étant donné que cela est nécessaire pour le point (b). De par les formules, on sait que :

$$
x \oplus y = fl(fl(x) + fl(y)) = (fl(x) + fl(y))(1 + \epsilon) = (x(1 + \epsilon') + y(1 + \epsilon''))(1 + \epsilon)
$$

On applique la distributivité pour rendre la mise en évidence de $(x + y)$ plus facile :

$$
(x(1 + \epsilon') + y(1 + \epsilon''))(1 + \epsilon) = (x + x\epsilon' + y + y\epsilon'')(1 + \epsilon) = x + x\epsilon' + y + y\epsilon'' + x\epsilon + x\epsilon'\epsilon + y\epsilon + y\epsilon''\epsilon = x + y + x\epsilon + x\epsilon' + y\epsilon + y\epsilon''
$$

On met $x + y$ en évidence en divisant tous les termes par $x + y$ :

$$
x + y + x\epsilon + x\epsilon' + y\epsilon + y\epsilon'' = 1(x + y) + \epsilon(x + y) + x\epsilon' + y\epsilon'' = (x + y)(1 + \epsilon + \frac{x\epsilon'}{x + y} + \frac{y\epsilon''}{x + y})
$$
