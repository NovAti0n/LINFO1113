# E1.2 - Errors and error propagation

## Question 1 - Truncation errors for series

How many term would we need to sum in the following series to garantee a trucation error bellow $10^{-10}$:

- $\sin(x) = x − \frac{x^3}{3!} + \frac{x^5}{5!} − … \text{for }x \in [−\pi, \pi]$
- $\exp(x) = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + … \text{for }x \in [−1, 1]$
- $\ln(1 + x) = x − \frac{x^2}{2} + \frac{x^3}{3} − \frac{x^4}{4} + … \text{for }x \in [0, 0.99]$

Your answer should have the following format: `r1, r2, r3`

**Hint**: create a small python script to compute these series with a predifined number $n$ of terms and compare the result with the actual values of $\sin$, $\exp$ and $\ln$.

### Solution

```
11, 14, 1495
```

### Explications

Il suffit, comme l'explique l'indice, de faire un script Python pour chaque cas afin de calculer à partir de combien de terme la différence entre la valeur réelle et l'approximation passe en dessous de $10^{-10}$.

Pour $\sin(x)$, on peut utiliser le code suivant :

```py
import numpy as np

def sin_series(x, n):
    result = 0
    sign = 1

    for i in range(1, 2 * n, 2):
        result += sign * (x ** i) / np.math.factorial(i)
        sign *= -1 # On inverse le signe

    return result

x = np.arange(-np.pi, np.pi + 0.01, 0.01) # Le deuxième argument est exclusif. On doit donc mettre np.pi + 0.1 pour être sur que pi soit dans le vecteur
n = 1

while not np.allclose(np.sin(x), np.vectorize(sin_series)(x, n), atol=1e-10, rtol=0):
    n += 1
        
print(f"Le n minimal est {n}.")
```

On obtient alors comme résultat que $n = 11$.

Pour $\exp(x)$, on répète le même processus :

```py
import numpy as np

def exp_series(x, n):
    result = 0

    for i in range(n):
        result += (x ** i) / np.math.factorial(i)

    return result

x = np.arange(-1, 1 + 0.01, 0.01)
n = 1

while not np.allclose(np.vectorize(exp_series)(x, n), np.exp(x), atol=1e-10, rtol=0):
    n += 1
        
print(f"Le n minimal est {n}.")
```

On obtient alors comme résultat $n = 14$.

Encore la même chose avec $\ln(x)$ :

```py
import numpy as np

def ln_series(x, n):
    result = 0
    sign = 1

    for i in range(1, n + 1):
        result += sign * (x ** i) / i
        sign *= -1 # On inverse le signe

    return result

x = np.arange(0, 0.99 + 0.01, 0.01)
n = 1

while not np.allclose(np.vectorize(ln_series)(x, n), np.log(1 + x), atol=1e-10, rtol=0):
    n += 1
        
print(f"Le n minimal est {n}.")
```

On obtient alors comme résultat $n = 1495$.

::: tip Précisions
Les fonctions [`np.allclose`](https://numpy.org/doc/stable/reference/generated/numpy.allclose.html) et [`np.vectorize`](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html) sont toutes les deux utilisées dans ces codes et sont très intéressantes. La première permet de vérifier si deux *arrays* ont tous leurs éléments égaux à une certaine tolérance près (définie par l'argument nommé `atol`. `rtol` quant à lui ne nous intéresse pas et doit donc être défini à 0, au risque de produire des résultats erronés). `np.sin` renvoie bien un *array*, mais ce n'est pas le cas pour la fonction `sin_series`. `np.vectorize` permet de réparer facilement le problème : elle prend une fonction `A` en argument et renvoie une autre fonction (que l'on appelle directement avec les arguments `x` et `n`) qui peut cette fois-ci prendre un *array* en argument et renvoyer un nouvel *array* dans lequel se trouveront tous les éléments auxquels on aura appliqué la fonction `A`. (Cela est équivalent à la fonction `map` d'une certaine manière)
:::

## Question 2 - Absolute and relative errors (1/4)

Assume that $\tilde{x}=0.937$ has three exact decimals with respect to $x$, meaning that $x \in [0.9365, 0.9375]$.

- What is the (maximal) absolute error on $x$?
- What is the (maximal) relative error on $x$ in %?

*Your answer should have the following format:* `r1, r2`

### Solution

```
0.0005, 0.05
```

### Explications

::: info Rappel
L'erreur absolue est donnée par $e_x \le |\tilde{x} - x|$ et l'erreur relative par $\epsilon_x \le \frac{|\tilde{x} - x|}{|x|}$, où dans les deux cas, $x$ correspond à la valeur réelle et $\tilde{x}$ à l'approximation (avec par conséquent $\tilde{x} = x + e_x$).
:::

Pour calculer l'erreur absolue maximale entre $x$ et $\tilde{x}$, il suffit de prendre les valeurs les plus éloignées et appliquer la formule. Dans ce cas, on peut prendre $x = 0.9365$ ou $x = 0.9375$, étant donné que ces valeurs sont éloignées de la même façon de $\tilde{x}$. On a donc :

$$\begin{align}
e_x \le |0.937 - 0.9365| = 0.0005 && e_x \le |0.937 - 0.9375| = 0.0005
\end{align}$$

L'erreur absolue maximale est donc de $0.0005$ avec $x = 0.9365$ ou $x = 0.9375$.

Le même principe est appliqué pour calculer l'erreur relative maximale. On a donc :

$$\begin{align}
\epsilon_x \le \frac{|0.937 - 0.9365|}{|0.9365|} \approx 0.0005 && \epsilon_x \le \frac{|0.937 - 0.9375|}{|0.9375|} \approx 0.0005
\end{align}$$

L'erreur relative maximale est donc de $0.0005 = 0.05\%$ avec $x = 0.9365$ ou $x = 0.9375$.

## Question 3 - Absolute and relative errors (2/4)

Suppose that $f(x) = \sqrt{1 − x}$ and $\tilde{x} = 0.937$.

- What is the (maximal) absolute error on $f(x)$?
- What is the (maximal) relative error on $f(x)$ in %?

*Your answer should have the following format:* `r1, r2`

### Solution

```
0.001, 0.4
```

### Explications

::: info Rappel
L'erreur absolue d'une fonction est donnée par $e_f \le |\frac{\partial f(x)}{\partial x}| * |e_x|$ et l'erreur relative par $\epsilon_x \le \frac{e_f}{f(x)}$.
:::

Pour calculer l'erreur absolue maximale, il suffit d'appliquer la formule avec la valeur nous ayant donné la plus grande erreur absolue dans l'exercice précédent, à savoir $x = 0.9365$ ou $x = 0.9375$. On a donc :

$$\begin{align}
e_f &\le |- \frac{1}{2\sqrt{1 - 0.9365}}| * 0.0005 & e_f &\le |- \frac{1}{2\sqrt{1 - 0.9375}}| * 0.0005\\
&\approx 0.001 & &= 0.001
\end{align}$$

On obtient une réponse légèrement inférieure à $0.01$ avec $x = 0.9365$ (cela est dû à la racine carrée). L'erreur maximale absolue est donc de $0.001$ avec $x = 0.9375$.

Le même principe est appliqué pour calculer l'erreur relative maximale. On va cette fois-ci uniquement le faire avec $x=0.9375$, étant donné que c'est celui qui donne la plus grande erreur avec cette fonction. On a donc :

$$\epsilon_x = \frac{0.001}{\sqrt{1 - 0.9375}} = \frac{0.001}{0.25} = 0.004$$

L'erreur relative maximale est donc de $0.004 = 0.4\%$ avec $x = 0.9375$.

## Question 4 - Absolute and relative errors (3/4)

Now, for the same function, assume that $\tilde{x} = 0.999$ and still has three exact decimals with respect to $x$.

- What is the (maximal) absolute error on $f(x)$?
- What is the (maximal) relative error on $f(x)$ in %?

*Your answer should have the following format:* `r1, r2`

### Solution

```
0.0111, 50
```

### Explications

Comme $\tilde{x}$ a changé, l'intervalle de $x$ également ($x \in [0.9995; 1]$. On s'arrête à 1 pour éviter d'avoir l'intérieur de la racine carrée négatif).

On réapplique les mêmes formules qu'avant, avec cette fois-ci $x = 0.9995$ :

$$\begin{align}
e_x &\le |0.999 - 0.9995| = 0.0005\\
e_f &\le |- \frac{1}{2\sqrt{1 - 0.9995}}| * 0.0005\\
&\approx 0.0111
\end{align}$$

L'erreur absolue maximale est $0.0111$ avec $x = 0.9995$.

Pour l'erreur relative maximale, on calcule :

$$\epsilon_x = \frac{0.0111}{\sqrt{1 - 0.9995}} = \frac{0.0111}{0.0224} = 0.496 \approx 0.5$$

L'erreur relative maximale est donc $0.5 = 50\%$ avec $x = 0.9995$.

## Question 5 - Absolute and relative errors (4/4)

Use *matplotlib* to plot on the same figure:

- $f(x)$
- $f(x) + |e_f(x)|$
- $f(x) − |e_f(x)|$

for $x \in [0.97, 1[$.

Use increments of $2 * 10^{−5}$ for your vector `x` and assume a constant $e_x = 10^{-3}$. Store the values of $f(x)$ and $e_f(x)$ in variables `f` and `e`.

### Solution

Comme pour tous les énoncés faisant intervenir Matplotlib, la solution est en deux parties :

- Le code (qui est à soumettre sur INGInious) 
- Le graphe (qui n'est pas corrigé par INGInious et qui est là à titre indicatif).

#### Code

```py:line-numbers
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(0.97, 1 - 2e-5, 2e-5)
f = np.sqrt(1 - x)
e = np.abs(-1 / (2 * np.sqrt(1 - x))) * 1e-3

plt.plot(x, f, "r", label="f(x)")
plt.plot(x, f + e, "g", label="f(x) + |e_f|")
plt.plot(x, f - e, "b", label="f(x) - |e_f|")

plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.show()
```

#### Graphe

![Graphe Matplotlib Q5](/images/e1-2-q5.png)

### Explications

Il suffit de créer le vecteur demandé avec la [fonction `arange`](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) et de le stocker dans `x`, puis de transformer la fonction et l'erreur absolue en Python, ce qui est fait avec Numpy.

::: danger Attention
Dans ce code précis, vous remarquerez sûrement que `arange` ne fonctionne pas de la bonne façon : le deuxième paramètre est censé être exclu de l'*array* produit. Néanmoins, si on met `1` au lieu de `1 - 2e-5`, la valeur 1 se retrouve dans l'*array*. Cela est dû à des erreurs causées par les nombres à virgules lorsque la prochaine valeur est trop proche de la précédente (comme ici, où l'on travaille avec des pas de $2 * 10^{-5}$). Il faut donc enlever l'équivalent d'un pas à la valeur, ce qu'on fait dans la fonction.
:::

On utilise ensuite la [fonction `plot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) pour tracer la fonction $f(x)$, la fonction $f(x) + |e_x|$ et la fonction $f(x) - |e_x|$. Les fonctions [`xlabel`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html), [`ylabel`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylabel.html) et [`legend`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html) sont utilisées pour respectivement donner un nom à l'axe des abscisses, donner un nom à l'axe des ordonnées et afficher la légende (en haut à droite sur le [graphe](#graphe)). Pour finir, on appelle la [fonction `show`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html) pour afficher le graphe.

## Question 6 - Error propagation on functions

How would you propagate the error $e_x$, $e_y$ and $e_z$ associated to $x$, $y$ and $z$ through the function:

$$
f(x, y, z) = \sin(x) * y^2 + \frac{z^3}{y} − x + xyz
$$

We expect an answer of the form: $|e_f| \leq$ `[your formula]`

Write your formula in a function `error(x, y, z, e_x, e_y, e_z)` that returns the value of the propagation, given the values of $x$, $y$, $z$, $e_x$, $e_y$ and $e_z$. Your function must respect $|e_f| \leq$ `error(...)`

**Hint**: You can use the functions `np.abs` from numpy to compute absolute values, `np.cos` and `np.sin` for trigonometric functions, and `a**b` to compute $a^b$

### Solution

```py:line-numbers
import numpy as np

def error(x, y, z, e_x, e_y, e_z):
    return np.abs(y ** 2 * np.cos(x) - 1 + y * z) * np.abs(e_x) + np.abs(2 * y * np.sin(x) - z ** 3 / y ** 2 + x * z) * np.abs(e_y) + np.abs(3 * z ** 2 / y + x * y) * np.abs(e_z)
```

### Explications

::: info Rappel
L'erreur absolue d'une fonction à plus d'une variable est donnée par la formule suivante :
$$e_x \le \sum_{i=1}^n |\frac{\partial f}{\partial x_i} (x_0, ..., x_n)| * |e_i| $$
:::

La formule présentée ci-dessus peut paraître compliquée, mais elle est simple à appliquer. Il suffit d'appliquer la formule classique pour une fonction à une variable à chaque variable et d'additionner le tout. Étape par étape, cela donne :

$$\begin{align}
e_{f(x)} &\le |\frac{\partial f(x, y, z)}{\partial x}| * |e_x|\\
&\le |y^2 cos(x) - 1 + yz| * |e_x|\\\\

e_{f(y)} &\le |\frac{\partial f(x, y, z)}{\partial y}| * |e_x|\\
&\le |2y sin(x) - \frac{z^3}{y^2} + xz| * |e_y|\\\\

e_{f(z)} &\le |\frac{\partial f(x, y, z)}{\partial z}| * |e_x|\\
&\le |\frac{3z^2}{y} + xy| * |e_z|

\end{align}$$

On additionne ensuite les fonctions obtenues pour avoir la fonction de l'erreur absolue :

$$e_{f} \le |y^2 cos(x) - 1 + yz| * |e_x| + |2y sin(x) - \frac{z^3}{y^2} + xz| * |e_y| + |\frac{3z^2}{y} + xy| * |e_z|$$

Il suffit ensuite de traduire cette formule en Python, ce qui est fait dans la solution.

## Question 7 - Variance computation (1/2)

You have a data set of $N$ points: $\{x_i|i = 1, ..., N\}$
.

You want to compute the variance ($V$) of the dataset. Knowing that the mean is computed as $\mu = \frac{1}{N} \sum_{i = 1}^N x_i$, which of the following formulas to compute the variance is the best to use if all the $x_i$ values are close to the mean?

- $V_1 = (\frac{1}{N} \sum_{i = 1}^N x_i^2) - \mu^2$
- $V_2 = \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2$

### Solution

$V_2 = \frac{1}{N} \sum_{i = 1}^N (x_i - \mu)^2$

### Explications

La formule $V_1 = (\frac{1}{N} \sum_{i = 1}^N x_i^2) - \mu^2$ est problématique car elle soustrait deux nombres à virgule très similaires (La moyenne des carrés est proche du carré de la moyenne, autrement dit $\frac{1}{N} \sum_{i = 1}^N x_i^2 \approx \mu^2$). On aura donc une perte de précision (cf. slide 64 du cours 1). $V_2$ est par conséquent préférable.

## Question 8 - Variance computation (2/2)

Test this with Python. Compute values `V1` and `V2` for $N = 7$ and $x_i = 10^{6.1} + i$ for $i \in \{1, …, 7\}$.

**Hint**: The difference is much more visible when using 32 bit values for your `x` vector instead of the default 64 bit representation. You can choose the representation using the `dtype = np.float32` as argument when defining an array. Don't hesitate to test the two to see what happens! Note that this may depend on how you computed your variances (the results might differ between implementations).

### Solution

```py:line-numbers
import numpy as np

x = 10 ** 6.1 + np.arange(1, 8, dtype=np.float32)
V1 = np.mean(x**2)- np.mean(x) ** 2
V2 = np.mean((x - np.mean(x)) ** 2)
```

### Explications

Cette question est une continuité de la question précédente, vu qu'elle va prouver la perte de précision dont $V1$ souffre. Il suffit de traduire en Python les deux formules. 

::: tip Remarque
$\frac{1}{N} \sum_{i=1}^N x_i$ est défini comme la moyenne de $x_i$, tout comme $\mu$. On peut donc les remplacer tous les deux par la [fonction `mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html).
:::

Après exécution, on obtient que $V1 = 185178.109375$ alors que $V2 = 4$ (qui est la bonne réponse). On voit clairement que $V1$ a subit un *overflow*, ce qui a fait exploser sa valeur et donne par conséquent un résultat complètement erroné.

## Question 9 - Operations errors

**(a)** Do the error propagation on the following float operation (we did multiplication and subtraction in the class room, see slide 63 to 67). See how the absolute (resp. relative) error on the operation depends on the absolute (resp. relative) error of each input parameter.

- $x \otimes y$
- $x \oslash y$
- $x \oplus y$
- $x \ominus y$
- $(x \oplus y) \oplus z$
- $(x \oplus z) \oplus y$

**Hint**:

- $fl(operation(x, y, z)) = operation(x, y, z)(1 + \epsilon)$
- $fl(x) = x(1 + \epsilon')$
- $fl(y) = y(1 + \epsilon'')$
- $fl(z) = z(1 + \epsilon''')$

**(b)** If $x = 1$, $y = 10^{−10}$, $z = −1$ and we are working with a number representation in base 10 with 10 decimals ($p$ = 10), can you numerically evaluate the relative error on:

- $(x \oplus y) \oplus z$
- $(x \oplus z) \oplus y$

**Hint**: In our representation (base 10, $p$ = 10), $(x \oplus y) \oplus z = 0$ and $(x \oplus z) \oplus y = 10^{−10}$.

*Give the answers of part* **(b)** *in % under the format:* `r1, r2`

### Solution

```
100, 0
```

### Explications