# E1.2 - Errors and error propagation

## Question 1 - Truncation errors for series

### Solution

```
11, 14, 1495
```

### Explications

Il suffit, comme l'explique l'indice, de faire un script Python pour chaque cas afin de calculer à partir de combien de terme la différence entre la valeur réelle et l'approximation passe en dessous de $10^{-10}$.

Pour $sin(x)$, on peut utiliser le code suivant :

```py
import numpy as np

def sin_series(x, n):
    result = 0
    sign = 1

    for i in range(1, 2 * n, 2):
        result += sign * (x ** i) / np.math.factorial(i)
        sign *= -1 # On inverse le signe

    return result

x = np.arange(-np.pi, np.pi + 0.01, 0.01) # Le deuxième argument est exclusif. On doit donc mettre np.pi + 0.1 pour être sur que pi soit dans le vecteur
n = 1

while not np.allclose(np.sin(x), np.vectorize(sin_series)(x, n), atol=1e-10, rtol=0):
    n += 1
        
print(f"Le n minimal est {n}.")
```

On obtient alors comme résultat que $N = 11$.

Pour $exp(x)$, on répète le même processus :

```py
import numpy as np

def exp_series(x, n):
    result = 0

    for i in range(n):
        result += (x ** i) / np.math.factorial(i)

    return result

x = np.arange(-1, 1 + 0.01, 0.01)
n = 1

while not np.allclose(np.vectorize(exp_series)(x, n), np.exp(x), atol=1e-10, rtol=0):
    n += 1
        
print(f"Le n minimal est {n}.")
```

On obtient alors comme résultat $N = 14$.

Encore la même chose avec $ln(x)$ :

```py
import numpy as np

def ln_series(x, n):
    result = 0
    sign = 1

    for i in range(1, n + 1):
        result += sign * (x ** i) / i
        sign *= -1 # On inverse le signe

    return result

x = np.arange(0, 0.99 + 0.01, 0.01)
n = 1

while not np.allclose(np.vectorize(ln_series)(x, n), np.log(1 + x), atol=1e-10, rtol=0):
    n += 1
        
print(f"Le n minimal est {n}.")
```

On obtient alors comme résultat $N = 1495$.

::: tip Précisions
Les fonctions [`np.allclose`](https://numpy.org/doc/stable/reference/generated/numpy.allclose.html) et [`np.vectorize`](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html) sont toutes les deux utilisées dans ces codes et sont très intéressantes. La première permet de vérifier si deux *arrays* ont tous leurs éléments égaux à une certaine tolérance près (définie par l'argument nommé `atol`. `rtol` quant à lui ne nous intéresse pas et doit donc être défini à 0, au risque de produire des résultats erronés). `np.sin` renvoie bien un *array*, mais ce n'est pas le cas pour la fonction `sin_series`. `np.vectorize` permet de réparer facilement le problème : elle prend une fonction `A` en argument et renvoie une autre fonction (que l'on appelle directement avec les arguments `x` et `n`) qui peut cette fois-ci prendre un *array* en argument et renvoyer un nouvel *array* dans lequel se trouveront tous les éléments auxquels on aura appliqué la fonction `A`. (Cela est équivalent à la fonction `map` d'une certaine manière)
:::

## Question 2 - Absolute and relative errors (1/4)

### Solution

```
0.0005, 0.05
```

### Explications

::: info Rappel
L'erreur absolue est donnée par $e_x \le |\tilde{x} - x|$ et l'erreur relative par $\epsilon_x \le \frac{|\tilde{x} - x|}{|x|}$, où dans les deux cas, $x$ correspond à la valeur réelle et $\tilde{x}$ à l'approximation (avec par conséquent $\tilde{x} = x + e_x$).
:::

Pour calculer l'erreur absolue maximale entre $x$ et $\tilde{x}$, il suffit de prendre les valeurs les plus éloignées et appliquer la formule. Dans ce cas, on peut prendre $x = 0.9365$ ou $x = 0.9375$, étant donné que ces valeurs sont éloignées de la même façon de $\tilde{x}$. On a donc :

$$\begin{align}
e_x \le |0.937 - 0.9365| = 0.0005 && e_x \le |0.937 - 0.9375| = 0.0005
\end{align}$$

L'erreur absolue maximale est donc de $0.0005$ avec $x = 0.9365$ ou $x = 0.9375$.

Le même principe est appliqué pour calculer l'erreur relative maximale. On a donc :

$$\begin{align}
\epsilon_x \le \frac{|0.937 - 0.9365|}{|0.9365|} \approx 0.0005 && \epsilon_x \le \frac{|0.937 - 0.9375|}{|0.9375|} \approx 0.0005
\end{align}$$

L'erreur relative maximale est donc de $0.0005 = 0.05\%$ avec $x = 0.9365$ ou $x = 0.9375$.

## Question 3 - Absolute and relative errors (2/4)

### Solution

```
0.001, 0.4
```

### Explications

::: info Rappel
L'erreur absolue d'une fonction est donnée par $e_f \le |\frac{\partial f(x)}{\partial x}| * |e_x|$ et l'erreur relative par $\epsilon_x \le \frac{e_f}{f(x)}$.
:::

Pour calculer l'erreur absolue maximale, il suffit d'appliquer la formule avec la valeur nous ayant donné la plus grande erreur absolue dans l'exercice précédent, à savoir $x = 0.9365$ ou $x = 0.9375$. On a donc :

$$\begin{align}
e_f &\le |- \frac{1}{2\sqrt{1 - 0.9365}}| * 0.0005 & e_f &\le |- \frac{1}{2\sqrt{1 - 0.9375}}| * 0.0005\\
&\approx 0.001 & &= 0.001
\end{align}$$

On obtient une réponse légèrement inférieure à $0.01$ avec $x = 0.9365$ (cela est dû à la racine carrée). L'erreur maximale absolue est donc de $0.001$ avec $x = 0.9375$.

Le même principe est appliqué pour calculer l'erreur relative maximale. On va cette fois-ci uniquement le faire avec $x=0.9375$, étant donné que c'est celui qui donne la plus grande erreur avec cette fonction. On a donc :

$$\epsilon_x = \frac{0.001}{\sqrt{1 - 0.9375}} = \frac{0.001}{0.25} = 0.004$$

L'erreur relative maximale est donc de $0.004 = 0.4\%$ avec $x = 0.9375$.

## Question 4 - Absolute and relative errors (3/4)

### Solution

```
0.0111, 50
```

### Explications

Comme $\tilde{x}$ a changé, l'intervalle de $x$ également ($x \in [0.9995; 1]$. On s'arrête à 1 pour éviter d'avoir l'intérieur de la racine carrée négatif).

On réapplique les mêmes formules qu'avant, avec cette fois-ci $x = 0.9995$ :

$$\begin{align}
e_x &\le |0.999 - 0.9995| = 0.0005\\
e_f &\le |- \frac{1}{2\sqrt{1 - 0.9995}}| * 0.0005\\
&\approx 0.0111
\end{align}$$

L'erreur absolue maximale est $0.0111$ avec $x = 0.9995$.

Pour l'erreur relative maximale, on calcule :

$$\epsilon_x = \frac{0.0111}{\sqrt{1 - 0.9995}} = \frac{0.0111}{0.0224} = 0.496 \approx 0.5$$

L'erreur relative maximale est donc $0.5 = 50\%$ avec $x = 0.9995$.

## Question 5 - Absolute and relative errors (4/4)

### Solution

Comme pour tous les énoncés faisant intervenir Matplotlib, la solution est en deux parties :
* Le code (qui est à soumettre sur INGInious) 
* Le graphe (qui n'est pas corrigé par INGInious et qui est là à titre indicatif).

#### Code

```py:line-numbers
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(0.97, 1 - 2e-5, 2e-5)
f = np.sqrt(1 - x)
e = np.abs(-1 / (2 * np.sqrt(1 - x))) * 1e-3

plt.plot(x, f, "r", label="f(x)")
plt.plot(x, f + e, "g", label="f(x) + |e_f|")
plt.plot(x, f - e, "b", label="f(x) - |e_f|")

plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.show()
```

#### Graphe

![Graphe Matplotlib Q5](/images/e1-2-q5.png)

### Explications

Il suffit de créer le vecteur demandé avec la [fonction `arange`](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) et de le stocker dans `x`, puis de transformer la fonction et l'erreur absolue en Python, ce qui est fait avec Numpy.

::: danger Attention
Dans ce code précis, vous remarquerez sûrement que `arange` ne fonctionne pas de la bonne façon : le deuxième paramètre est censé être exclu de l'*array* produit. Néanmoins, si on met `1` au lieu de `1 - 2e-5`, la valeur 1 se retrouve dans l'*array*. Cela est dû à des erreurs causées par les nombres à virgules lorsque la prochaine valeur est trop proche de la précédente (comme ici, où l'on travaille avec des pas de $2 * 10^{-5}$). Il faut donc enlever l'équivalent d'un pas à la valeur, ce qu'on fait dans la fonction.
:::

On utilise ensuite la [fonction `plot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) pour tracer la fonction $f(x)$, la fonction $f(x) + |e_x|$ et la fonction $f(x) - |e_x|$. Les fonctions [`xlabel`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html), [`ylabel`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylabel.html) et [`legend`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html) sont utilisées pour respectivement donner un nom à l'axe des abscisses, donner un nom à l'axe des ordonnées et afficher la légende (en haut à droite sur le [graphe](#graphe)). Pour finir, on appelle la [fonction `show`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html) pour afficher le graphe.

## Question 6 - Error propagation on functions

### Solution

```py:line-numbers
import numpy as np

def error(x, y, z, e_x, e_y, e_z):
    return np.abs(y ** 2 * np.cos(x) - 1 + y * z) * np.abs(e_x) + np.abs(2 * y * np.sin(x) - z ** 3 / y ** 2 + x * z) * np.abs(e_y) + np.abs(3 * z ** 2 / y + x * y) * np.abs(e_z)
```

### Explications

::: info Rappel
L'erreur absolue d'une fonction à plus d'une variable est donnée par la formule suivante :
$$e_x \le \sum_{i=1}^n |\frac{\partial f}{\partial x_i} (x_0, ..., x_n)| * |e_i| $$
:::

La formule présentée ci-dessus peut paraître compliquée, mais elle est simple à appliquer. Il suffit d'appliquer la formule classique pour une fonction à une variable à chaque variable et d'additionner le tout. Étape par étape, cela donne :

$$\begin{align}
e_{f(x)} &\le |\frac{\partial f(x, y, z)}{\partial x}| * |e_x|\\
&\le |y^2 cos(x) - 1 + yz| * |e_x|\\\\

e_{f(y)} &\le |\frac{\partial f(x, y, z)}{\partial y}| * |e_x|\\
&\le |2y sin(x) - \frac{z^3}{y^2} + xz| * |e_y|\\\\

e_{f(z)} &\le |\frac{\partial f(x, y, z)}{\partial z}| * |e_x|\\
&\le |\frac{3z^2}{y} + xy| * |e_z|

\end{align}$$

On additionne ensuite les fonctions obtenues pour avoir la fonction de l'erreur absolue :

$$e_{f} \le |y^2 cos(x) - 1 + yz| * |e_x| + |2y sin(x) - \frac{z^3}{y^2} + xz| * |e_y| + |\frac{3z^2}{y} + xy| * |e_z|$$

Il suffit ensuite de traduire cette formule en Python, ce qui est fait dans la solution.

## Question 7 - Variance computation (1/2)

### Solution

$V_2 = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2$

### Explications

La formule $V1=(\frac{1}{N} \sum_{i=1}^N x_i^2) - \mu^2$ est problématique car elle soustrait deux nombres à virgule très similaires (La moyenne des carrés est proche du carré de la moyenne, autrement dit $\frac{1}{N} \sum_{i=1}^N x_i^2 \approx \mu^2$). On aura donc une perte de précision (cf. slide 64 du cours 1). $V_2$ est par conséquent préférable.

## Question 8 - Variance computation (2/2)

### Solution

```py:line-numbers
import numpy as np

x = 10 ** 6.1 + np.arange(1, 8, dtype=np.float32)
V1 = np.mean(x**2)- np.mean(x) ** 2
V2 = np.mean((x - np.mean(x)) ** 2)
```

### Explications

Cette question est une continuité de la question précédente, vu qu'elle va prouver la perte de précision dont $V1$ souffre. Il suffit de traduire en Python les deux formules. 

::: tip Remarque
$\frac{1}{N} \sum_{i=1}^N x_i$ est défini comme la moyenne de $x_i$, tout comme $\mu$. On peut donc les remplacer tous les deux par la [fonction `mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html).
:::

Après exécution, on obtient que $V1 = 185178.109375$ alors que $V2 = 4$ (qui est la bonne réponse). On voit clairement que $V1$ a subit un *overflow*, ce qui a fait exploser sa valeur et donne par conséquent un résultat complètement erroné.

## Question 9 - Operations errors

### Solution

```
100, 0
```

### Explications